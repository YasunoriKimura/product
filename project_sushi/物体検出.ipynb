{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "物体検出.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ok9WeFHp2L3",
        "colab_type": "text"
      },
      "source": [
        "# YOLO3を使用して寿司の物体検出をする\n",
        "\n",
        "・(320, 320)にリサイズした画像</br>\n",
        "・アノテーションしたテキストを用意</br>\n",
        "事前にkeras-yolo3を使用できるようにしておく。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hH__hSRXkV2E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh680sPwk4bN",
        "colab_type": "code",
        "outputId": "441af857-7016-4358-8638-8213975aad55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd  drive/My Drive/project_sushi/keras-yolo3"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/project_sushi/keras-yolo3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNylQAga72pz",
        "colab_type": "text"
      },
      "source": [
        "###自前データで学習<br>\n",
        "####keras-yolo3/train.pyを編集<br>\n",
        "```\n",
        "annotation_path = '../annotation_resize.txt'\n",
        "classes_path = '../labelImg/data/predefined_classes.txt'\n",
        "input_shape = (320,320) \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfE7QCWB8rby",
        "colab_type": "code",
        "outputId": "7f70eb19-8745-4d50-dbfa-c5c0cc0f5f30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3267
        }
      },
      "source": [
        "#確認\n",
        "!cat train.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"\"\"\n",
            "Retrain the YOLO model for your own dataset.\n",
            "\"\"\"\n",
            "\n",
            "import numpy as np\n",
            "import keras.backend as K\n",
            "from keras.layers import Input, Lambda\n",
            "from keras.models import Model\n",
            "from keras.optimizers import Adam\n",
            "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
            "\n",
            "from yolo3.model import preprocess_true_boxes, yolo_body, tiny_yolo_body, yolo_loss\n",
            "from yolo3.utils import get_random_data\n",
            "\n",
            "\n",
            "def _main():\n",
            "    annotation_path = '../annotation_resize.txt'\n",
            "    log_dir = 'logs/000/'\n",
            "    classes_path = '../labelImg/data/predefined_classes.txt'\n",
            "    anchors_path = 'model_data/yolo_anchors.txt'\n",
            "    class_names = get_classes(classes_path)\n",
            "    num_classes = len(class_names)\n",
            "    anchors = get_anchors(anchors_path)\n",
            "\n",
            "    input_shape = (320,320) # multiple of 32, hw\n",
            "\n",
            "    is_tiny_version = len(anchors)==6 # default setting\n",
            "    if is_tiny_version:\n",
            "        model = create_tiny_model(input_shape, anchors, num_classes,\n",
            "            freeze_body=2, weights_path='model_data/tiny_yolo_weights.h5')\n",
            "    else:\n",
            "        model = create_model(input_shape, anchors, num_classes,\n",
            "            freeze_body=2, weights_path='model_data/yolo_weights.h5') # make sure you know what you freeze\n",
            "\n",
            "    logging = TensorBoard(log_dir=log_dir)\n",
            "    checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',\n",
            "        monitor='val_loss', save_weights_only=True, save_best_only=True, period=3)\n",
            "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)\n",
            "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)\n",
            "\n",
            "    val_split = 0.1\n",
            "    with open(annotation_path) as f:\n",
            "        lines = f.readlines()\n",
            "    np.random.seed(10101)\n",
            "    np.random.shuffle(lines)\n",
            "    np.random.seed(None)\n",
            "    num_val = int(len(lines)*val_split)\n",
            "    num_train = len(lines) - num_val\n",
            "\n",
            "    # Train with frozen layers first, to get a stable loss.\n",
            "    # Adjust num epochs to your dataset. This step is enough to obtain a not bad model.\n",
            "    if True:\n",
            "        model.compile(optimizer=Adam(lr=1e-3), loss={\n",
            "            # use custom yolo_loss Lambda layer.\n",
            "            'yolo_loss': lambda y_true, y_pred: y_pred})\n",
            "\n",
            "        batch_size = 32\n",
            "        print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
            "        model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n",
            "                steps_per_epoch=max(1, num_train//batch_size),\n",
            "                validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
            "                validation_steps=max(1, num_val//batch_size),\n",
            "                epochs=50,\n",
            "                initial_epoch=0,\n",
            "                callbacks=[logging, checkpoint])\n",
            "        model.save_weights(log_dir + 'trained_weights_stage_1.h5')\n",
            "\n",
            "    # Unfreeze and continue training, to fine-tune.\n",
            "    # Train longer if the result is not good.\n",
            "    if True:\n",
            "        for i in range(len(model.layers)):\n",
            "            model.layers[i].trainable = True\n",
            "        model.compile(optimizer=Adam(lr=1e-4), loss={'yolo_loss': lambda y_true, y_pred: y_pred}) # recompile to apply the change\n",
            "        print('Unfreeze all of the layers.')\n",
            "\n",
            "        batch_size = 32 # note that more GPU memory is required after unfreezing the body\n",
            "        print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
            "        model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n",
            "            steps_per_epoch=max(1, num_train//batch_size),\n",
            "            validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
            "            validation_steps=max(1, num_val//batch_size),\n",
            "            epochs=100,\n",
            "            initial_epoch=50,\n",
            "            callbacks=[logging, checkpoint, reduce_lr, early_stopping])\n",
            "        model.save_weights(log_dir + 'trained_weights_final.h5')\n",
            "\n",
            "    # Further training if needed.\n",
            "\n",
            "\n",
            "def get_classes(classes_path):\n",
            "    '''loads the classes'''\n",
            "    with open(classes_path) as f:\n",
            "        class_names = f.readlines()\n",
            "    class_names = [c.strip() for c in class_names]\n",
            "    return class_names\n",
            "\n",
            "def get_anchors(anchors_path):\n",
            "    '''loads the anchors from a file'''\n",
            "    with open(anchors_path) as f:\n",
            "        anchors = f.readline()\n",
            "    anchors = [float(x) for x in anchors.split(',')]\n",
            "    return np.array(anchors).reshape(-1, 2)\n",
            "\n",
            "\n",
            "def create_model(input_shape, anchors, num_classes, load_pretrained=True, freeze_body=2,\n",
            "            weights_path='model_data/yolo_weights.h5'):\n",
            "    '''create the training model'''\n",
            "    K.clear_session() # get a new session\n",
            "    image_input = Input(shape=(None, None, 3))\n",
            "    h, w = input_shape\n",
            "    num_anchors = len(anchors)\n",
            "\n",
            "    y_true = [Input(shape=(h//{0:32, 1:16, 2:8}[l], w//{0:32, 1:16, 2:8}[l], \\\n",
            "        num_anchors//3, num_classes+5)) for l in range(3)]\n",
            "\n",
            "    model_body = yolo_body(image_input, num_anchors//3, num_classes)\n",
            "    print('Create YOLOv3 model with {} anchors and {} classes.'.format(num_anchors, num_classes))\n",
            "\n",
            "    if load_pretrained:\n",
            "        model_body.load_weights(weights_path, by_name=True, skip_mismatch=True)\n",
            "        print('Load weights {}.'.format(weights_path))\n",
            "        if freeze_body in [1, 2]:\n",
            "            # Freeze darknet53 body or freeze all but 3 output layers.\n",
            "            num = (185, len(model_body.layers)-3)[freeze_body-1]\n",
            "            for i in range(num): model_body.layers[i].trainable = False\n",
            "            print('Freeze the first {} layers of total {} layers.'.format(num, len(model_body.layers)))\n",
            "\n",
            "    model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss',\n",
            "        arguments={'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.5})(\n",
            "        [*model_body.output, *y_true])\n",
            "    model = Model([model_body.input, *y_true], model_loss)\n",
            "\n",
            "    return model\n",
            "\n",
            "def create_tiny_model(input_shape, anchors, num_classes, load_pretrained=True, freeze_body=2,\n",
            "            weights_path='model_data/tiny_yolo_weights.h5'):\n",
            "    '''create the training model, for Tiny YOLOv3'''\n",
            "    K.clear_session() # get a new session\n",
            "    image_input = Input(shape=(None, None, 3))\n",
            "    h, w = input_shape\n",
            "    num_anchors = len(anchors)\n",
            "\n",
            "    y_true = [Input(shape=(h//{0:32, 1:16}[l], w//{0:32, 1:16}[l], \\\n",
            "        num_anchors//2, num_classes+5)) for l in range(2)]\n",
            "\n",
            "    model_body = tiny_yolo_body(image_input, num_anchors//2, num_classes)\n",
            "    print('Create Tiny YOLOv3 model with {} anchors and {} classes.'.format(num_anchors, num_classes))\n",
            "\n",
            "    if load_pretrained:\n",
            "        model_body.load_weights(weights_path, by_name=True, skip_mismatch=True)\n",
            "        print('Load weights {}.'.format(weights_path))\n",
            "        if freeze_body in [1, 2]:\n",
            "            # Freeze the darknet body or freeze all but 2 output layers.\n",
            "            num = (20, len(model_body.layers)-2)[freeze_body-1]\n",
            "            for i in range(num): model_body.layers[i].trainable = False\n",
            "            print('Freeze the first {} layers of total {} layers.'.format(num, len(model_body.layers)))\n",
            "\n",
            "    model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss',\n",
            "        arguments={'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.7})(\n",
            "        [*model_body.output, *y_true])\n",
            "    model = Model([model_body.input, *y_true], model_loss)\n",
            "\n",
            "    return model\n",
            "\n",
            "def data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes):\n",
            "    '''data generator for fit_generator'''\n",
            "    n = len(annotation_lines)\n",
            "    i = 0\n",
            "    while True:\n",
            "        image_data = []\n",
            "        box_data = []\n",
            "        for b in range(batch_size):\n",
            "            if i==0:\n",
            "                np.random.shuffle(annotation_lines)\n",
            "            image, box = get_random_data(annotation_lines[i], input_shape, random=True)\n",
            "            image_data.append(image)\n",
            "            box_data.append(box)\n",
            "            i = (i+1) % n\n",
            "        image_data = np.array(image_data)\n",
            "        box_data = np.array(box_data)\n",
            "        y_true = preprocess_true_boxes(box_data, input_shape, anchors, num_classes)\n",
            "        yield [image_data, *y_true], np.zeros(batch_size)\n",
            "\n",
            "def data_generator_wrapper(annotation_lines, batch_size, input_shape, anchors, num_classes):\n",
            "    n = len(annotation_lines)\n",
            "    if n==0 or batch_size<=0: return None\n",
            "    return data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes)\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    _main()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0esQe9zlYe1",
        "colab_type": "code",
        "outputId": "6913e85a-596b-41a3-eadd-507efc2987a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4236
        }
      },
      "source": [
        "#学習\n",
        "!python3 train.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0618 14:21:08.030790 139779087677312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:95: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n",
            "W0618 14:21:08.031187 139779087677312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0618 14:21:08.067797 139779087677312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0618 14:21:08.068064 139779087677312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0618 14:21:08.072033 139779087677312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "2019-06-18 14:21:08.129876: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-06-18 14:21:08.132159: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2a4b2c0 executing computations on platform Host. Devices:\n",
            "2019-06-18 14:21:08.132197: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-06-18 14:21:08.137370: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-06-18 14:21:08.377534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-18 14:21:08.378119: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2a4bd40 executing computations on platform CUDA. Devices:\n",
            "2019-06-18 14:21:08.378150: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-06-18 14:21:08.378375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-18 14:21:08.378774: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-06-18 14:21:08.390583: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-18 14:21:08.571670: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-06-18 14:21:08.652349: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-06-18 14:21:08.672190: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-06-18 14:21:08.869460: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-06-18 14:21:08.978579: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-06-18 14:21:09.353722: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-06-18 14:21:09.353987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-18 14:21:09.354567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-18 14:21:09.354984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-06-18 14:21:09.358079: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-18 14:21:09.359896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-18 14:21:09.359928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-06-18 14:21:09.359940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-06-18 14:21:09.361649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-18 14:21:09.362192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-18 14:21:09.362617: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-06-18 14:21:09.362661: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "W0618 14:21:10.968794 139779087677312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0618 14:21:16.025042 139779087677312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
            "\n",
            "Create YOLOv3 model with 9 anchors and 10 classes.\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_59 due to mismatch in shape ((1, 1, 1024, 45) vs (255, 1024, 1, 1)).\n",
            "  weight_values[i].shape))\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_59 due to mismatch in shape ((45,) vs (255,)).\n",
            "  weight_values[i].shape))\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_67 due to mismatch in shape ((1, 1, 512, 45) vs (255, 512, 1, 1)).\n",
            "  weight_values[i].shape))\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_67 due to mismatch in shape ((45,) vs (255,)).\n",
            "  weight_values[i].shape))\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_75 due to mismatch in shape ((1, 1, 256, 45) vs (255, 256, 1, 1)).\n",
            "  weight_values[i].shape))\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_75 due to mismatch in shape ((45,) vs (255,)).\n",
            "  weight_values[i].shape))\n",
            "Load weights model_data/yolo_weights.h5.\n",
            "Freeze the first 249 layers of total 252 layers.\n",
            "W0618 14:21:27.068430 139779087677312 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3080: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0618 14:21:30.335125 139779087677312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Train on 186 samples, val on 20 samples, with batch size 32.\n",
            "W0618 14:21:32.027904 139779087677312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "W0618 14:21:32.028206 139779087677312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "Epoch 1/50\n",
            "2019-06-18 14:22:07.223351: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] shape_optimizer failed: Invalid argument: Subshape must have computed start >= end since stride is negative, but is 0 and 2 (computed from start 0 and end 9223372036854775807 over shape with rank 2 and stride-1)\n",
            "2019-06-18 14:22:07.298241: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] remapper failed: Invalid argument: Subshape must have computed start >= end since stride is negative, but is 0 and 2 (computed from start 0 and end 9223372036854775807 over shape with rank 2 and stride-1)\n",
            "2019-06-18 14:22:07.561611: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] layout failed: Invalid argument: Subshape must have computed start >= end since stride is negative, but is 0 and 2 (computed from start 0 and end 9223372036854775807 over shape with rank 2 and stride-1)\n",
            "2019-06-18 14:22:07.991279: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] shape_optimizer failed: Invalid argument: Subshape must have computed start >= end since stride is negative, but is 0 and 2 (computed from start 0 and end 9223372036854775807 over shape with rank 2 and stride-1)\n",
            "2019-06-18 14:22:08.062731: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] remapper failed: Invalid argument: Subshape must have computed start >= end since stride is negative, but is 0 and 2 (computed from start 0 and end 9223372036854775807 over shape with rank 2 and stride-1)\n",
            "2019-06-18 14:22:09.225802: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-06-18 14:22:12.539165: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "4/5 [=======================>......] - ETA: 19s - loss: 5112.93262019-06-18 14:23:06.505364: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] remapper failed: Invalid argument: Subshape must have computed start >= end since stride is negative, but is 0 and 2 (computed from start 0 and end 9223372036854775807 over shape with rank 2 and stride-1)\n",
            "2019-06-18 14:23:06.607541: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] layout failed: Invalid argument: Subshape must have computed start >= end since stride is negative, but is 0 and 2 (computed from start 0 and end 9223372036854775807 over shape with rank 2 and stride-1)\n",
            "2019-06-18 14:23:06.738253: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] remapper failed: Invalid argument: Subshape must have computed start >= end since stride is negative, but is 0 and 2 (computed from start 0 and end 9223372036854775807 over shape with rank 2 and stride-1)\n",
            "5/5 [==============================] - 94s 19s/step - loss: 4870.3860 - val_loss: 3427.7090\n",
            "Epoch 2/50\n",
            "5/5 [==============================] - 17s 3s/step - loss: 2766.2286 - val_loss: 1898.7911\n",
            "Epoch 3/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 1543.7647 - val_loss: 1091.5645\n",
            "Epoch 4/50\n",
            "5/5 [==============================] - 3s 567ms/step - loss: 915.7349 - val_loss: 658.4578\n",
            "Epoch 5/50\n",
            "5/5 [==============================] - 3s 579ms/step - loss: 589.0931 - val_loss: 426.1894\n",
            "Epoch 6/50\n",
            "5/5 [==============================] - 4s 801ms/step - loss: 415.0752 - val_loss: 333.0431\n",
            "Epoch 7/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 320.6491 - val_loss: 271.1869\n",
            "Epoch 8/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 255.5552 - val_loss: 209.3963\n",
            "Epoch 9/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 217.1700 - val_loss: 197.5371\n",
            "Epoch 10/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 189.3074 - val_loss: 171.2055\n",
            "Epoch 11/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 164.5077 - val_loss: 148.8715\n",
            "Epoch 12/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 155.4561 - val_loss: 141.3519\n",
            "Epoch 13/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 146.2056 - val_loss: 133.8307\n",
            "Epoch 14/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 131.6787 - val_loss: 116.8492\n",
            "Epoch 15/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 123.5828 - val_loss: 120.1934\n",
            "Epoch 16/50\n",
            "5/5 [==============================] - 7s 1s/step - loss: 117.8968 - val_loss: 114.6336\n",
            "Epoch 17/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 109.5904 - val_loss: 105.9041\n",
            "Epoch 18/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 107.0975 - val_loss: 98.9968\n",
            "Epoch 19/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 102.1806 - val_loss: 91.9927\n",
            "Epoch 20/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 99.3680 - val_loss: 96.4010\n",
            "Epoch 21/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 89.4285 - val_loss: 93.4127\n",
            "Epoch 22/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 86.3135 - val_loss: 89.5567\n",
            "Epoch 23/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 84.4929 - val_loss: 84.5942\n",
            "Epoch 24/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 84.7086 - val_loss: 76.9376\n",
            "Epoch 25/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 80.7126 - val_loss: 78.4950\n",
            "Epoch 26/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 78.3608 - val_loss: 77.3357\n",
            "Epoch 27/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 72.0996 - val_loss: 75.5291\n",
            "Epoch 28/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 71.4583 - val_loss: 69.9825\n",
            "Epoch 29/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 68.5112 - val_loss: 68.9904\n",
            "Epoch 30/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 70.6927 - val_loss: 68.1965\n",
            "Epoch 31/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 68.0500 - val_loss: 65.5560\n",
            "Epoch 32/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 65.5979 - val_loss: 60.4564\n",
            "Epoch 33/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 61.8292 - val_loss: 65.0390\n",
            "Epoch 34/50\n",
            "5/5 [==============================] - 7s 1s/step - loss: 62.1451 - val_loss: 61.6057\n",
            "Epoch 35/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 59.9382 - val_loss: 63.4723\n",
            "Epoch 36/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 60.9002 - val_loss: 54.4044\n",
            "Epoch 37/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 60.6978 - val_loss: 60.2505\n",
            "Epoch 38/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 57.6907 - val_loss: 55.2954\n",
            "Epoch 39/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 54.8021 - val_loss: 51.8410\n",
            "Epoch 40/50\n",
            "5/5 [==============================] - 7s 1s/step - loss: 54.6673 - val_loss: 51.9664\n",
            "Epoch 41/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 53.2797 - val_loss: 54.9719\n",
            "Epoch 42/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 52.4960 - val_loss: 49.8473\n",
            "Epoch 43/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 52.5848 - val_loss: 49.8199\n",
            "Epoch 44/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 50.6456 - val_loss: 51.1663\n",
            "Epoch 45/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 49.1216 - val_loss: 51.7756\n",
            "Epoch 46/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 49.9759 - val_loss: 48.8731\n",
            "Epoch 47/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 47.5226 - val_loss: 48.9489\n",
            "Epoch 48/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 47.8649 - val_loss: 47.9958\n",
            "Epoch 49/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 45.9600 - val_loss: 48.1446\n",
            "Epoch 50/50\n",
            "5/5 [==============================] - 8s 2s/step - loss: 45.2675 - val_loss: 47.1132\n",
            "Unfreeze all of the layers.\n",
            "Train on 186 samples, val on 20 samples, with batch size 32.\n",
            "Epoch 51/100\n",
            "2019-06-18 14:30:44.103967: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] shape_optimizer failed: Invalid argument: Subshape must have computed start >= end since stride is negative, but is 0 and 2 (computed from start 0 and end 9223372036854775807 over shape with rank 2 and stride-1)\n",
            "2019-06-18 14:30:44.917398: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] remapper failed: Invalid argument: Subshape must have computed start >= end since stride is negative, but is 0 and 2 (computed from start 0 and end 9223372036854775807 over shape with rank 2 and stride-1)\n",
            "2019-06-18 14:30:47.803758: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] layout failed: Invalid argument: Subshape must have computed start >= end since stride is negative, but is 0 and 2 (computed from start 0 and end 9223372036854775807 over shape with rank 2 and stride-1)\n",
            "2019-06-18 14:30:51.011989: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] shape_optimizer failed: Invalid argument: Subshape must have computed start >= end since stride is negative, but is 0 and 2 (computed from start 0 and end 9223372036854775807 over shape with rank 2 and stride-1)\n",
            "2019-06-18 14:30:51.561989: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] remapper failed: Invalid argument: Subshape must have computed start >= end since stride is negative, but is 0 and 2 (computed from start 0 and end 9223372036854775807 over shape with rank 2 and stride-1)\n",
            "2019-06-18 14:30:57.487213: W tensorflow/core/common_runtime/bfc_allocator.cc:237] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
            "2019-06-18 14:30:57.491232: W tensorflow/core/common_runtime/bfc_allocator.cc:237] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
            "2019-06-18 14:30:57.683900: W tensorflow/core/common_runtime/bfc_allocator.cc:237] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.32GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
            "2019-06-18 14:30:57.683997: W tensorflow/core/common_runtime/bfc_allocator.cc:237] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.32GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
            "4/5 [=======================>......] - ETA: 6s - loss: 34.0398 2019-06-18 14:31:06.825594: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] remapper failed: Invalid argument: Subshape must have computed start >= end since stride is negative, but is 0 and 2 (computed from start 0 and end 9223372036854775807 over shape with rank 2 and stride-1)\n",
            "2019-06-18 14:31:07.083087: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] layout failed: Invalid argument: Subshape must have computed start >= end since stride is negative, but is 0 and 2 (computed from start 0 and end 9223372036854775807 over shape with rank 2 and stride-1)\n",
            "2019-06-18 14:31:07.463489: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] remapper failed: Invalid argument: Subshape must have computed start >= end since stride is negative, but is 0 and 2 (computed from start 0 and end 9223372036854775807 over shape with rank 2 and stride-1)\n",
            "5/5 [==============================] - 32s 6s/step - loss: 32.9399 - val_loss: 36.1517\n",
            "Epoch 52/100\n",
            "5/5 [==============================] - 7s 1s/step - loss: 26.1887 - val_loss: 31.2443\n",
            "Epoch 53/100\n",
            "5/5 [==============================] - 7s 1s/step - loss: 23.8944 - val_loss: 28.1865\n",
            "Epoch 54/100\n",
            "5/5 [==============================] - 7s 1s/step - loss: 22.6799 - val_loss: 26.8876\n",
            "Epoch 55/100\n",
            "5/5 [==============================] - 8s 2s/step - loss: 21.9551 - val_loss: 23.6234\n",
            "Epoch 56/100\n",
            "5/5 [==============================] - 9s 2s/step - loss: 20.9998 - val_loss: 24.4311\n",
            "Epoch 57/100\n",
            "5/5 [==============================] - 9s 2s/step - loss: 20.5051 - val_loss: 21.9524\n",
            "Epoch 58/100\n",
            "5/5 [==============================] - 9s 2s/step - loss: 19.5432 - val_loss: 22.6121\n",
            "Epoch 59/100\n",
            "5/5 [==============================] - 9s 2s/step - loss: 19.4077 - val_loss: 21.8565\n",
            "Epoch 60/100\n",
            "5/5 [==============================] - 9s 2s/step - loss: 18.8421 - val_loss: 21.6737\n",
            "Epoch 61/100\n",
            "5/5 [==============================] - 9s 2s/step - loss: 18.0850 - val_loss: 21.7405\n",
            "Epoch 62/100\n",
            "5/5 [==============================] - 9s 2s/step - loss: 17.7658 - val_loss: 21.2644\n",
            "Epoch 63/100\n",
            "5/5 [==============================] - 9s 2s/step - loss: 17.7360 - val_loss: 19.6193\n",
            "Epoch 64/100\n",
            "5/5 [==============================] - 9s 2s/step - loss: 17.7291 - val_loss: 22.7615\n",
            "Epoch 65/100\n",
            "5/5 [==============================] - 9s 2s/step - loss: 17.5902 - val_loss: 20.4990\n",
            "Epoch 66/100\n",
            "5/5 [==============================] - 9s 2s/step - loss: 17.5299 - val_loss: 21.5945\n",
            "\n",
            "Epoch 00066: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
            "Epoch 67/100\n",
            "5/5 [==============================] - 7s 1s/step - loss: 17.4017 - val_loss: 20.1342\n",
            "Epoch 68/100\n",
            "5/5 [==============================] - 9s 2s/step - loss: 16.7903 - val_loss: 20.7065\n",
            "Epoch 69/100\n",
            "5/5 [==============================] - 9s 2s/step - loss: 16.1967 - val_loss: 21.0082\n",
            "\n",
            "Epoch 00069: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
            "Epoch 70/100\n",
            "5/5 [==============================] - 9s 2s/step - loss: 16.8257 - val_loss: 19.9531\n",
            "Epoch 71/100\n",
            "5/5 [==============================] - 9s 2s/step - loss: 16.1813 - val_loss: 21.2397\n",
            "Epoch 72/100\n",
            "5/5 [==============================] - 9s 2s/step - loss: 16.6271 - val_loss: 20.9504\n",
            "\n",
            "Epoch 00072: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
            "Epoch 73/100\n",
            "5/5 [==============================] - 9s 2s/step - loss: 16.5057 - val_loss: 20.1545\n",
            "Epoch 00073: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwCH0RPgp0d4",
        "colab_type": "text"
      },
      "source": [
        "### 学習したモデルを使用して物体検出をする\n",
        "#### yolo.pyを編集\n",
        "```\n",
        "\"model_path\": 'logs/000/trained_weights_stage_1.h5',\n",
        "\"anchors_path\": 'model_data/yolo_anchors.txt',\n",
        "\"classes_path\": '../labelImg/data/predefined_classes.txt',\n",
        "\"model_image_size\" : (320, 320),\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoutOKwXleHD",
        "colab_type": "code",
        "outputId": "dc549521-2771-43a9-fc17-07dec2d77242",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3621
        }
      },
      "source": [
        "#確認\n",
        "!cat yolo.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"\n",
            "Class definition of YOLO_v3 style detection model on image and video\n",
            "\"\"\"\n",
            "\n",
            "import colorsys\n",
            "import os\n",
            "from timeit import default_timer as timer\n",
            "\n",
            "import numpy as np\n",
            "from keras import backend as K\n",
            "from keras.models import load_model\n",
            "from keras.layers import Input\n",
            "from PIL import Image, ImageFont, ImageDraw\n",
            "\n",
            "from yolo3.model import yolo_eval, yolo_body, tiny_yolo_body\n",
            "from yolo3.utils import letterbox_image\n",
            "import os\n",
            "from keras.utils import multi_gpu_model\n",
            "\n",
            "class YOLO(object):\n",
            "    _defaults = {\n",
            "        \"model_path\": 'logs/000/trained_weights_final.h5',\n",
            "        \"anchors_path\": 'model_data/yolo_anchors.txt',\n",
            "        \"classes_path\": '../labelImg/data/predefined_classes.txt',\n",
            "        \"score\" : 0.3,\n",
            "        \"iou\" : 0.45,\n",
            "        \"model_image_size\" : (320, 320),\n",
            "        \"gpu_num\" : 1,\n",
            "    }\n",
            "\n",
            "    @classmethod\n",
            "    def get_defaults(cls, n):\n",
            "        if n in cls._defaults:\n",
            "            return cls._defaults[n]\n",
            "        else:\n",
            "            return \"Unrecognized attribute name '\" + n + \"'\"\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.__dict__.update(self._defaults) # set up default values\n",
            "        self.__dict__.update(kwargs) # and update with user overrides\n",
            "        self.class_names = self._get_class()\n",
            "        self.anchors = self._get_anchors()\n",
            "        self.sess = K.get_session()\n",
            "        self.boxes, self.scores, self.classes = self.generate()\n",
            "\n",
            "    def _get_class(self):\n",
            "        classes_path = os.path.expanduser(self.classes_path)\n",
            "        with open(classes_path) as f:\n",
            "            class_names = f.readlines()\n",
            "        class_names = [c.strip() for c in class_names]\n",
            "        return class_names\n",
            "\n",
            "    def _get_anchors(self):\n",
            "        anchors_path = os.path.expanduser(self.anchors_path)\n",
            "        with open(anchors_path) as f:\n",
            "            anchors = f.readline()\n",
            "        anchors = [float(x) for x in anchors.split(',')]\n",
            "        return np.array(anchors).reshape(-1, 2)\n",
            "\n",
            "    def generate(self):\n",
            "        model_path = os.path.expanduser(self.model_path)\n",
            "        assert model_path.endswith('.h5'), 'Keras model or weights must be a .h5 file.'\n",
            "\n",
            "        # Load model, or construct model and load weights.\n",
            "        num_anchors = len(self.anchors)\n",
            "        num_classes = len(self.class_names)\n",
            "        is_tiny_version = num_anchors==6 # default setting\n",
            "        try:\n",
            "            self.yolo_model = load_model(model_path, compile=False)\n",
            "        except:\n",
            "            self.yolo_model = tiny_yolo_body(Input(shape=(None,None,3)), num_anchors//2, num_classes) \\\n",
            "                if is_tiny_version else yolo_body(Input(shape=(None,None,3)), num_anchors//3, num_classes)\n",
            "            self.yolo_model.load_weights(self.model_path) # make sure model, anchors and classes match\n",
            "        else:\n",
            "            assert self.yolo_model.layers[-1].output_shape[-1] == \\\n",
            "                num_anchors/len(self.yolo_model.output) * (num_classes + 5), \\\n",
            "                'Mismatch between model and given anchor and class sizes'\n",
            "\n",
            "        print('{} model, anchors, and classes loaded.'.format(model_path))\n",
            "\n",
            "        # Generate colors for drawing bounding boxes.\n",
            "        hsv_tuples = [(x / len(self.class_names), 1., 1.)\n",
            "                      for x in range(len(self.class_names))]\n",
            "        self.colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
            "        self.colors = list(\n",
            "            map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)),\n",
            "                self.colors))\n",
            "        np.random.seed(10101)  # Fixed seed for consistent colors across runs.\n",
            "        np.random.shuffle(self.colors)  # Shuffle colors to decorrelate adjacent classes.\n",
            "        np.random.seed(None)  # Reset seed to default.\n",
            "\n",
            "        # Generate output tensor targets for filtered bounding boxes.\n",
            "        self.input_image_shape = K.placeholder(shape=(2, ))\n",
            "        if self.gpu_num>=2:\n",
            "            self.yolo_model = multi_gpu_model(self.yolo_model, gpus=self.gpu_num)\n",
            "        boxes, scores, classes = yolo_eval(self.yolo_model.output, self.anchors,\n",
            "                len(self.class_names), self.input_image_shape,\n",
            "                score_threshold=self.score, iou_threshold=self.iou)\n",
            "        return boxes, scores, classes\n",
            "\n",
            "    def detect_image(self, image):\n",
            "        start = timer()\n",
            "\n",
            "        if self.model_image_size != (None, None):\n",
            "            assert self.model_image_size[0]%32 == 0, 'Multiples of 32 required'\n",
            "            assert self.model_image_size[1]%32 == 0, 'Multiples of 32 required'\n",
            "            boxed_image = letterbox_image(image, tuple(reversed(self.model_image_size)))\n",
            "        else:\n",
            "            new_image_size = (image.width - (image.width % 32),\n",
            "                              image.height - (image.height % 32))\n",
            "            boxed_image = letterbox_image(image, new_image_size)\n",
            "        image_data = np.array(boxed_image, dtype='float32')\n",
            "\n",
            "        print(image_data.shape)\n",
            "        image_data /= 255.\n",
            "        image_data = np.expand_dims(image_data, 0)  # Add batch dimension.\n",
            "\n",
            "        out_boxes, out_scores, out_classes = self.sess.run(\n",
            "            [self.boxes, self.scores, self.classes],\n",
            "            feed_dict={\n",
            "                self.yolo_model.input: image_data,\n",
            "                self.input_image_shape: [image.size[1], image.size[0]],\n",
            "                K.learning_phase(): 0\n",
            "            })\n",
            "\n",
            "        print('Found {} boxes for {}'.format(len(out_boxes), 'img'))\n",
            "\n",
            "        font = ImageFont.truetype(font='font/FiraMono-Medium.otf',\n",
            "                    size=np.floor(3e-2 * image.size[1] + 0.5).astype('int32'))\n",
            "        thickness = (image.size[0] + image.size[1]) // 300\n",
            "\n",
            "        for i, c in reversed(list(enumerate(out_classes))):\n",
            "            predicted_class = self.class_names[c]\n",
            "            box = out_boxes[i]\n",
            "            score = out_scores[i]\n",
            "\n",
            "            label = '{} {:.2f}'.format(predicted_class, score)\n",
            "            draw = ImageDraw.Draw(image)\n",
            "            label_size = draw.textsize(label, font)\n",
            "\n",
            "            top, left, bottom, right = box\n",
            "            top = max(0, np.floor(top + 0.5).astype('int32'))\n",
            "            left = max(0, np.floor(left + 0.5).astype('int32'))\n",
            "            bottom = min(image.size[1], np.floor(bottom + 0.5).astype('int32'))\n",
            "            right = min(image.size[0], np.floor(right + 0.5).astype('int32'))\n",
            "            print(label, (left, top), (right, bottom))\n",
            "\n",
            "            if top - label_size[1] >= 0:\n",
            "                text_origin = np.array([left, top - label_size[1]])\n",
            "            else:\n",
            "                text_origin = np.array([left, top + 1])\n",
            "\n",
            "            # My kingdom for a good redistributable image drawing library.\n",
            "            for i in range(thickness):\n",
            "                draw.rectangle(\n",
            "                    [left + i, top + i, right - i, bottom - i],\n",
            "                    outline=self.colors[c])\n",
            "            draw.rectangle(\n",
            "                [tuple(text_origin), tuple(text_origin + label_size)],\n",
            "                fill=self.colors[c])\n",
            "            draw.text(text_origin, label, fill=(0, 0, 0), font=font)\n",
            "            del draw\n",
            "\n",
            "        end = timer()\n",
            "        print(end - start)\n",
            "        return image\n",
            "\n",
            "    def close_session(self):\n",
            "        self.sess.close()\n",
            "\n",
            "def detect_video(yolo, video_path, output_path=\"\"):\n",
            "    import cv2\n",
            "    vid = cv2.VideoCapture(video_path)\n",
            "    if not vid.isOpened():\n",
            "        raise IOError(\"Couldn't open webcam or video\")\n",
            "    video_FourCC    = int(vid.get(cv2.CAP_PROP_FOURCC))\n",
            "    video_fps       = vid.get(cv2.CAP_PROP_FPS)\n",
            "    video_size      = (int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
            "                        int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
            "    isOutput = True if output_path != \"\" else False\n",
            "    if isOutput:\n",
            "        print(\"!!! TYPE:\", type(output_path), type(video_FourCC), type(video_fps), type(video_size))\n",
            "        out = cv2.VideoWriter(output_path, video_FourCC, video_fps, video_size)\n",
            "    accum_time = 0\n",
            "    curr_fps = 0\n",
            "    fps = \"FPS: ??\"\n",
            "    prev_time = timer()\n",
            "    while True:\n",
            "        return_value, frame = vid.read()\n",
            "        image = Image.fromarray(frame)\n",
            "        image = yolo.detect_image(image)\n",
            "        result = np.asarray(image)\n",
            "        curr_time = timer()\n",
            "        exec_time = curr_time - prev_time\n",
            "        prev_time = curr_time\n",
            "        accum_time = accum_time + exec_time\n",
            "        curr_fps = curr_fps + 1\n",
            "        if accum_time > 1:\n",
            "            accum_time = accum_time - 1\n",
            "            fps = \"FPS: \" + str(curr_fps)\n",
            "            curr_fps = 0\n",
            "        cv2.putText(result, text=fps, org=(3, 15), fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
            "                    fontScale=0.50, color=(255, 0, 0), thickness=2)\n",
            "        cv2.namedWindow(\"result\", cv2.WINDOW_NORMAL)\n",
            "        cv2.imshow(\"result\", result)\n",
            "        if isOutput:\n",
            "            out.write(result)\n",
            "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
            "            break\n",
            "    yolo.close_session()\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbJssfuCpN1a",
        "colab_type": "code",
        "outputId": "c21f77c4-32d6-44b2-d43c-b0c68e23e671",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1193
        }
      },
      "source": [
        "!python3 yolo_video.py --image"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "Image detection mode\n",
            " Ignoring remaining command line arguments: ./path2your_video,\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0618 14:43:36.643421 139743442257792 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0618 14:43:36.643773 139743442257792 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0618 14:43:36.643935 139743442257792 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-06-18 14:43:36.658714: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-06-18 14:43:36.658941: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2fcb2c0 executing computations on platform Host. Devices:\n",
            "2019-06-18 14:43:36.658989: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-06-18 14:43:36.661002: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-06-18 14:43:36.842899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-18 14:43:36.843461: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2fcbd40 executing computations on platform CUDA. Devices:\n",
            "2019-06-18 14:43:36.843518: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-06-18 14:43:36.843892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-18 14:43:36.844271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-06-18 14:43:36.844566: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-18 14:43:36.845793: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-06-18 14:43:36.846985: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-06-18 14:43:36.847307: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-06-18 14:43:36.848745: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-06-18 14:43:36.849799: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-06-18 14:43:36.852957: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-06-18 14:43:36.853093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-18 14:43:36.853509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-18 14:43:36.853847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-06-18 14:43:36.853909: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-18 14:43:36.854735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-18 14:43:36.854763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-06-18 14:43:36.854771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-06-18 14:43:36.855071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-18 14:43:36.855492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-18 14:43:36.855839: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-06-18 14:43:36.855907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "W0618 14:43:36.856585 139743442257792 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "W0618 14:43:36.877923 139743442257792 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0618 14:43:37.471770 139743442257792 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0618 14:43:42.581415 139743442257792 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
            "\n",
            "logs/000/trained_weights_final.h5 model, anchors, and classes loaded.\n",
            "W0618 14:43:48.103745 139743442257792 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Input image filename:../test/test_maguro.jpeg\n",
            "(320, 320, 3)\n",
            "2019-06-18 14:44:30.083210: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] remapper failed: Invalid argument: Subshape must have computed start >= end since stride is negative, but is 0 and 2 (computed from start 0 and end 9223372036854775807 over shape with rank 2 and stride-1)\n",
            "2019-06-18 14:44:30.172660: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] layout failed: Invalid argument: Subshape must have computed start >= end since stride is negative, but is 0 and 2 (computed from start 0 and end 9223372036854775807 over shape with rank 2 and stride-1)\n",
            "2019-06-18 14:44:30.306597: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] remapper failed: Invalid argument: Subshape must have computed start >= end since stride is negative, but is 0 and 2 (computed from start 0 and end 9223372036854775807 over shape with rank 2 and stride-1)\n",
            "2019-06-18 14:44:30.644654: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "2019-06-18 14:44:30.729824: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-06-18 14:44:32.411916: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "Found 2 boxes for img\n",
            "maguro 0.72 (65, 76) (247, 171)\n",
            "maguro 0.98 (0, 74) (164, 157)\n",
            "3.7024060150001787\n",
            "Input image filename:Traceback (most recent call last):\n",
            "  File \"yolo_video.py\", line 92, in <module>\n",
            "    detect_img(YOLO(**vars(FLAGS)))\n",
            "  File \"yolo_video.py\", line 24, in detect_img\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E79YdcbEE9c9",
        "colab_type": "text"
      },
      "source": [
        "### 出力された画像を確認"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5_MLpVlCwdu",
        "colab_type": "code",
        "outputId": "cbb6ecc7-190f-491c-f374-4ae19ca6f1d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "from IPython.display import Image,display_jpeg\n",
        "display_jpeg(Image('out.jpg'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAIBAQEBAQIBAQECAgICAgQDAgICAgUEBAMEBgUGBgYF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\nkrVaf/iW1qWbHOM1qatNJqum/wCmndQHtDm4Z+KKjvIkz0orM2P/2Q==\n"
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogjRnGY-FOf-",
        "colab_type": "text"
      },
      "source": [
        "検出されてますね"
      ]
    }
  ]
}